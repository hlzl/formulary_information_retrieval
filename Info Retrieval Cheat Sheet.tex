% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% LaTeX4EI Example for Cheat Sheets
%
% @encode: 	UTF-8, tabwidth = 4, newline = LF
% @author:	LaTeX4EI
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % %


% ======================================================================
% Document Settings
% ======================================================================

% possible options: color/nocolor, english/german, threecolumn
% default: color, english
\documentclass[english]{latex4ei/latex4ei_sheet}
\usepackage{dsfont}
\usepackage{textcomp}
\usepackage{bbm}
% set document information
\title{Information \\Retrieval in High\\ Dimensional Data}


% DOCUMENT_BEGIN ===============================================================
\begin{document}

\maketitle	% requires ./img/Logo.pdf

\section{Basics}
\begin{sectionbox}
Observation Matrix: $\mathbf{X} \subset \R^{pxn}$ with $p$-dimensional random variable and $n$ obeservations ($i$: variable, $j$: observation)\\
Expected Value $\mu_X=\sum_{i\in X} x_i\cdot P_X(x_i)=E[X]\in\R^p$ \\
Variance $\Sigma=Var(X)=E[(X-\mu)(X-\mu)^{\top}]\in\R^{pxp}$\\
$\Sigma$ is symmetric ($\Sigma^{\top} = \Sigma$) and positive semidefinite: $\forall x$: $x^{\top}\Sigma x \ge 0\ $ or $\Sigma \ge 0$\\
Estimated Value based on mean of observations $n$: \\ $\hat{\mu}_p=\frac{1}{n}\sum_{j=1}^nx_{pj}$ (calculate for every dimension $p$)\\
Centered Observation Matrix $\hat{X}=X-\hat{\mu}\otimes [1\,...\,1]$ \\
Covariance Matrix $\hat{\Sigma}=\frac{1}{n-1}\hat{X}\hat{X}^{\top}\approx \frac{1}{n}\hat{X}\hat{X}^{\top}$ \\
Transpose Matrix $(XY)^{\top}=Y^{\top}X^{\top}$\\
\subsection{Random Variables}
Probability: $Pr(X\in\mathcal{X})=\int_\mathcal{X}p_X(x)dx=\sum_{\{i|x_i\in\mathcal{X}\}}p_i$\\
Marginal Density: $p_X(x)=\int_{\R^k}p_{X,Y}(x,y)dy$\\
Conditional Density: $p_{Y|X=x}(y)=\frac{p_{X,Y}(x,y)}{p_X(x)}$, with $\sum_{y}p_{Y|X=x}(y)=1$ \\
Expectation Value: $E[X]=\int_{\R^p}xp_X(x)dx=\mu_X$\\
(Co)Variance: $Var[X]=E[(X-\mu_X)(X-\mu_X)^{\top}$
\end{sectionbox}

\section{Statistical Decision Making}
\begin{sectionbox}
\subsection{Loss Function}
\begin{itemize}
    \item Quadratic Loss Function: $L(Y, f(X))=(Y-f(X))^2$
    \item Minimize Expected Prediction Error: $EPE(f) = \E[L(Y,f(X)]$
    \item If using quadratic loss function, conditional mean is best
    \item If using absolute loss $L(Y, f(X))=\vert(Y-f(X))\vert$, conditional median is best
\end{itemize}

\subsection{Decision Making}
Map an input vector to an output value and find a decision function that minimizes the deviation between a target and the output, i.e. the loss.
\begin{itemize}
    \item \textbf{Expected Prediction Error} as a function of $f$ is formulated by the expectation of the loss function, whose values change depending on the decision function $f$: $EPE(f)=E[L(Y, f(X))]$.
    \item The aim of \textbf{global methods} is to find the best explicit global function $\hat{f}=argmin_{f\in\mathcal{F}} EPE(f)$.
    \item The aim of \textbf{local methods} is to find the best output $\hat{c}=argmin_{c\in\R}\mathbb{E}_{Y|X=x}L(Y,c)$, i.e. minimizes the expectation of loss on the distribution of $Y$ conditioned on known $X$ (only samples in region of interest).
    \item \textbf{Binary Decision Making} can be visualised via a Joint PDF of $X$ and $Y$, where $Y$ takes on -1 or 1 and $X$  is a continuous random variable whose distribution changes conditioned on $Y$.
    \item We can obtain the Expected Prediction Error and Expected Loss due to the assumption that we \emph{know the Joint PDF} of $X$ and $Y$, i.e. the stochastic behaviour of the system.
    \item For local methods with loss specified as the squared loss, expected loss is found to be the conditional expectation of $Y$ on the distribution of $Y$ conditioned on $X=x$: $f(X)=\mathbb{E}_{Y|X=x}[Y]$.
    \item The problem with local methods is most of the time $X$ is a continuous random variable and thus values here $X=x$ are impossible, thus we take the \textbf{k-nearest neighbours} to $x$ as approximations of $x$. This gives us a set of samples representing the distribution of values $X=x$ such that local methods for finding the output can now be applied to these samples.
\end{itemize}
\end{sectionbox}

\begin{sectionbox}

\subsection{Generalization Error}
Difference between Empirical Loss and Expected Loss for a function $f$.\\In practice, the difference between the training and the validation error.
\begin{itemize}
    \item $G_N(f)=\mathbb{E}[L(Y,f(X))]-\frac{1}{N}\sideset{}{_{i=1}^N}\sum L(y_i,f(x_i))$
    \item $G_N \to 0$ for $N\to \infty$
\end{itemize}

\subsection{$k$-nearest Neighbors}
Problem: among many observations $(x_i,y_i)$ there is probably none with $x_i=x$.
\begin{itemize}
    \item Solvable by taking all observations into account which are near the desired $x$: $\hat{f}=\mathrm{average}(y_i|x_i \in N_k(x))$
    \item If the set of neighbors $k$ increases, the average tends towards the expectation value
    \item $x_i$ comes closer to $x$ if number of observations $N$ increases
    \item In practice $N$ is limited, thus imposing a model assumption for linear regression or "reducing" the dimesion of $X$ is needed.
\end{itemize}
\subsection{Curse of Dimensionality}
Most of the time it is desired that the dimensionality of samples are decreased due to various problems involved with working in high dimensions.\\
With increasing dimensions $p$:
\begin{itemize}
    \item Noise increases (accumulates over dimensions)
    \item The number of samples required to obtain an accurate representation of the probability distribution also increases a lot/exponentially.
    \item \emph{Empty Space Phenomenon}: High dimensional spaces are sparse and samples tend to be located at the tail end of their distributions.
    \item Distance between a point and any possible samples becomes more and more equidistant.
    \item For a random vector $X\in\R^p$, the probability of at least one $X_i$ being further than $\beta$ away from the center:
    \end{itemize}
    \begin{emphbox}
    $Pr(||X||^2_2\ge\beta)=1-Pr(X_1^2<\beta)^p$. 
    \end{emphbox}
    For large $p$ this becomes 1 and it's difficult to estimate the underlying distribution.\\

\subsection{Data Preparation}
\begin{itemize}
    \item \textbf{No}minal Categories - \textbf{No} ordering, \textbf{Or}dinal Categories: Ordering
    \item Num to Cat: Discretization, Cat to Num: Binarization
    \item \emph{Bag of Words}: Frequent and Distinct = strong Weight
    \item Frequent: Term Frequency
    \item Distinct: Inverse Document Frequency
    \item Text Prep: Remove HTML, lower case, Remove punctuation/numbers/common words, split into words
\end{itemize}
\end{sectionbox}

\section{Convex Functions}
\begin{sectionbox}
\begin{emphbox}
    \textbf{Definition Convexity}: $\mathcal{C}\subset\R^n$ a convex set, i.e for any pair $\mathbf{x_1,x_2}\in\mathcal{C}$ the point $t\mathbf{x_2}+(1-t)\mathbf{x_1} \in \mathcal{C} \ \forall t\in[0,1]$. A function is called \emph{convex} if $\forall \mathbf{x_1,x_2}\in \mathcal{C}, t\in[0,1]$:\\
    $tf(\mathbf{x_2})+(1-t)f(\mathbf{x_1}) \ge f(t\mathbf{x_2}+(1-t)\mathbf{x_1}) $
\end{emphbox}
\begin{itemize}
\item By definition, a function is convex if a line can be drawn between any two points in a function, and all vertical values of the line are greater than or equal to all vertical values of the function between the same two points.
\item A function is also convex if its second derivative is positive, or equivalently for vector inputs its Hessian matrix contains all non-negative eigenvalues.
\item Several operations between convex functions also result in another convex function, including the max between two convex functions:
\end{itemize}
\begin{emphbox}
    if $f$ and $g$ are convex, then $max(f,g)$, $f+g$ and $g\circ h$ (if $g$ non-decreasing) are convex. A local minimum of a \emph{strictly} convex function is a global minimum and unique.
\end{emphbox}
\end{sectionbox}
\vspace{50}
\section{Logistic Regression}
\begin{sectionbox}
\subsection{Formulation via Optimization}
\begin{itemize}
\item The aim of logistic regression to perform binary classification on a random vector, that is, map a random vector to either -1 or 1.
\item In order to ensure that the outputs in logistic regression are either -1 or 1, the output in logistic regression is taken to be the sign of the output from a prediction function f which contributes to the decision.
\item The loss function in logistic regression is taken to be the number of misclassifications of a prediction, that is, 1 if the decision from logistic regression does not match the target, and 0 if it does match.
$f(z) = \left\{ \begin{array}{rcl}
     1 & \mbox{if}
& Y\cdot sign(f(x))\leq 0 \\ 0 & & \mbox{otherwise}\\
\end{array}\right.$
\item It is clear that we must find an appropriate f  to complete the decision function, and thus we define the global method with the \textbf{misclassification loss} as the problem we need to solve.
$ \frac{1}{n}\sum\limits_{i=1}^nL_{0,1}(y_i,f(x_i))$
\item The misclassification loss is extremely difficult to solve due to being non-continuous and non-convex, and so first we need to approximate this function as a convex function since they are easy to optimize.
\item A convex approximation of misclassification loss is $log(1 + exp(-t))$, which when concatenated with  an affine function $f(\mathbf{x})=\mathbf{w^{\top}x}+b$ is also convex, and can therefore be easily optimized via minimization.\\
\begin{center}
\begin{emphbox}
    $\min\limits_{\mathbf{w}\in\R^n,b\in\R}\frac{1}{n}\sum\limits_{i=1}^nlog(1+exp(-y_i(\mathbf{w}^{\top}\mathbf{x}_i+b)))$
    \end{emphbox}
\end{center}
\item Noting that the loss is in \textbf{negative log-likelihood} form, we can convert this to a probability by taking the exponential of likelihood.
\begin{emphbox}
    \begin{center}
        $Pr(Y=y|x)=\frac{1}{1+exp(-y(\mathbf{w}^{\top}\mathbf{x}+b))}$
    \end{center}
\end{emphbox}
\item Gradient-based methods can be used to find affine weights $\mathbf{w}$ and bias $b$ that solve the optimization problem (\emph{first box}).
\item Classify new sample: $sign(\mathbf{w}^{T}\mathbf{x}_{new}+b)$.
\item Decision boundary of LR is always a hyperplane (i.e. an affine space) in $\mathbb{R}^p$.
\item Retraining with a new training sample affects the decision boundary.
\end{itemize}

\subsection{Overfitting on Logistic Regression}
\begin{itemize}
\item When the samples are linearly separable (\emph{a dividing hyperplane exists}), there is a constraint that the target times the affine output is always greater than zero Under this constraint, no global minimum exists.
\begin{emphbox}
    If the data is linearly separable, we can find ($\mathbf{w}_s,b_s$) so that: $\forall i:\,y_i(\mathbf{w}_s^{\top}\mathbf{x}_i+b_s)<0$ \\
    then the loss-function has no global minimum in $\R^{p+1}$
\end{emphbox}
This can be fixed by introducing a regulator which penalizes the magnitude of $(\mathbf{w}, b)$, e.g. by fixing a real constant $\lambda>0$ and adjusting the original cost function as  $\hat{F}(\mathbf{w}, b)=F(\mathbf{w}, b)+\lambda||\mathbf{w}||^2+b^2$.
\end{itemize}

\subsection{Alternative Approach (Statistics)}
Linear Model: $a = w_0+w_1x_1+...+w_nx_m = \mathbf{w}^{\top}\mathbf{x}$\\
\begin{tabular}{ll}
    Probability: $\sigma(a)=\frac{1}{1+e^{-a}}$ & $D=\{(\mathbf{x}_i,z_i)\}$\\
    Find $\mathbf{w}_{MLE}=arg \max\limits_{\mathbf{w}} P(D|\mathbf{w})$ &  $\mathbf{x}_i\in \R^d, z_i\in\{0,1\}$\\
\end{tabular}
$P(D|\mathbf{w})=\prod\limits_{i=1}^n\sigma(\mathbf{w}^{\top}\mathbf{x}_i)^{z_i}(1-\sigma(\mathbf{w}^{\top}\mathbf{x}_i))^{1-z_i}$\\
$L(\mathbf{w})=-log(P(D|\mathbf{w}))$, $\nabla_{\mathbf{w}}L(\mathbf{w})=\mathbf{X}(\sigma(\mathbf{X}^{\top}\mathbf{w})-\mathbf{z})$\\
(Requires Hessian Matrix, which is positive semi-definite)\\
Results are identical up to the factor 1/n
\end{sectionbox}

\section{Kernel Method}
\begin{sectionbox}
In its simplest form, the kernel trick means transforming data into another dimension that has a clear dividing margin between classes of data. The Kernel trick simply computes the inner products between the images of all pairs of data in the feature space to learn a nonlinear function or decision boundary.
\begin{emphbox}
    A positive semidefinite Kernel is a function $\kappa: \R^{p}\times \R^{p}\rightarrow\R$ such that for all finite sets $\mathbf{X}=\{\mathbf{x}_1,\cdot,\mathbf{x_n}\}$ the \emph{Gram-Matrix} $\mathbf{K}_{ij} = \kappa(\mathbf{x_i,x_j})$ is symmetric and positive semidefinite  
\end{emphbox}
\begin{itemize}
    \item Symmetric $\forall i,j: \kappa(\mathbf{x}_j,\mathbf{x}_i)=\kappa(\mathbf{x}_j,\mathbf{x}_i)$
    \item Positive semidefinite $\forall \mathbf{x}$: $\mathbf{x}^{\top}\mathbf{Kx}\ge 0\ $ or all eigenvalues $\ge 0$
    \item We can determine whether a particular function is a kernel by testing for violations of symmetry or positive semidefiniteness.
    Symmetry can easily be tested by substituting named variables into the kernel, then flipping around the variables and seeing whether the kernel expression is the same.
    \item Positive semidefiniteness can be tested as violated by definition, or if any diagonal values are ever negative. Furthermore, if the determinant of a matrix is negative it is definitely not positive semidefinite (since positive semidefinite eigenvalues are non-negative and the determinant is the product of the eigenvalues), but we cannot say anything if the determinant is positive.
\end{itemize}
\subsection{Common Kernels and Rules}
\begin{itemize}
    \item Linear kernel: $\kappa(\mathbf{x,y})=\mathbf{x^{\top}y}+c, \ c\ge0$
    \item Polynomial Kernel $\kappa(\mathbf{x,y})=(a\mathbf{x^{\top}y}+c)^d,\ a,c,d\ge0 $
    \item Gaussian Kernel $\kappa(\mathbf{x,y})=exp(-\frac{||\mathbf{x-y}||^2}{2\sigma^2})$, $\sigma>0$
    \item Exponential Kernel $\kappa(\mathbf{x,y})=exp(-\frac{||\mathbf{x-y}||^2}{2\sigma})$, $\sigma>0$
    \item If $\kappa_1, \kappa_2$ are Kernels and $c\ge0$, then $c\kappa_1$, $c+\kappa_1$, $\kappa_1+\kappa_2$ and $\kappa_1\cdot\kappa_2$ are kernels as well
\end{itemize}
\end{sectionbox}
\textbf{Notes:}

\newpage
\section{Principal Component Analysis}
\begin{sectionbox}
Assuming distribution of unlabeled raw data is concentrated around some lower-dimensional plane and a projection onto this plane captures most of the variance of the dataset. PCA uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\\
\subsection{Geometric Interpretation}
\begin{itemize}
\item Imagining that $x$ is a vector in $p$ dimensions, and $U_k$ describes a lower dimensional subspace, then the coordinates of $x$ projected onto $U_k$ is found by first obtaining the scores of $x$ along $U_k$ and then taking the linear combination of these scores and the $U_k$ vectors: $\pi_\mathcal{U}(\mathbf{x})=\mathbf{U}_k\mathbf{U}_k^{\top}\mathbf{x}, \ \mathbf{U}_k\in\R^{p\times k}$
\item We want to find a $U_k$ that captures most of the variance in the dataset, which is equivalent to finding a hyperplane where the difference between the original data points and their projections are minimized, thus forming our problem for optimization.\\
\begin{center}
    $J(\mathbf{U}_k)=\sum\limits_{i=1}^n||\mathbf{x}_i-\mathbf{U}_k\mathbf{U}_k^{\top}\mathbf{x}_i||^2_2$
\end{center}
\item The dataset can be represented as the dot product of an orthonormal matrix, a diagonal matrix and another orthonormal matrix via singular value decomposition: $\mathbf{X}=\mathbf{UDV}^{\top}$.
\end{itemize}
\begin{emphbox}
    The first $k$ columns of the first orthonormal matrix via SVD $\mathbf{U}_k$ minimizes the difference between data points and their projections, and the corresponding covariance matrix of the scores is diagonal (= features are uncorrelated).
\end{emphbox}
\begin{itemize}
\item The \emph{empirical covariance matrix} or \emph{Score Matrix} of the reduced $k$ variables $\mathbf{S}=\mathbf{U}_k^{\top}\mathbf{X}$ is diagonal, i.e. the features extracted by the PCA are uncorrelated.        [$cov(\mathbf{S})=\frac{1}{n}\mathbf{S}\mathbf{S}^{\top}=\frac{1}{n}\mathbf{U}^{\top}\mathbf{X}\mathbf{X}^{\top}\mathbf{U}$]\\
$s_{ij}$: $j$-th score for the $i$-th principal component
\item $U_k$ is the Loading Matrix give by $k$ left-singular vectors of the SVD of $\mathbf{\hat{X}}$.
\item $\mathbf{S}$ can directly be obtained by the SVD with $\mathbf{S}=\mathbf{D}_k\mathbf{V}_k^{\top}$. As $\mathbf{D_k}$ is diagonal and $\mathbf{V}_k^{\top}$ only has $k$-rows, this only requires $nk$ operations instead of non-feasible $(2p-1)nk$ operations.
\item \emph{Dimensions}:\\ $X\in\R^{m\times n}$, $U\in\R^{m\times m}$, $D\in\R^{m\times n}$, $V^{\top}\in\R^{n\times n}$, $U_k\in\R^{k\times n}$, $X_k\in\R^{k\times n}$, $m$: Dimension variable, $n$: samples
\item The eigenvectors of $\mathbf{X}\mathbf{X}^{\top}$ are the loadings of $\mathbf{X}$ only if $\mathbf{X}$ is centered.
\end{itemize}

\subsection{Proof}
\begin{itemize}
\item Theorem 4.1 is proved by first reframing the minimization problem (4.2) into a maximization problem, reframing this again into a problem involving the trace of a rank k projection matrix, then noting that the maximum value along the diagonals is 1 (4.5) of which subbing in $U_k$ achieves due to its resulting in a identity matrix.
We can show that the scores are uncorrelated by showing that the dot product between the scores and its transpose scaled by 1/n is a diagonal matrix using the SVD. (4.6)
\item If we have the product between two matrices where the first matrix is comprised of a diagonal matrix as well as a zero matrix, then we can get rid of the zero columns in the first matrix and also get rid of the corresponding rows in the left matrix.
\item A computationally inexpensive way of obtaining the scores using the singular values and right singular vector can be derived by substiuting the SVD into the formula for the calculating the scores. (4.7)
\end{itemize}

\subsection{Statistical Interpretation}
The idea behind the statistical interpretation is that covariance matrices are symmetric positive semidefinite, and therefore there exists some matrix $U$ which can diagonalize it (clearly illustrated using the eigenvalue decomposition). I.e. $Y=\mathbf{U}^{\top}X$ with $\mathbf{U}^{\top}\mathbf{U}=I_p$, such that the covariance matrix of $Y$ is diagonal and the variances decrease. In the same way:
\begin{emphbox}
        $\mathbf{D}=\mathbf{U}^{\top}var(X)\mathbf{U} $
        $= \mathbb{E}[\mathbf{U}^{\top}(x-\mu_x)(X-\mu_x)^{\top}\mathbf{U}]$
        $=\mathbb{E}[(\mathbf{U}^{\top}X-\mathbf{U}^{\top}\mu_x)(\mathbf{U}^{\top}X-\mathbf{U}^{\top}\mu_x)^{\top}]$
        $=\mathbb{E}[(Y-\mu_y)(Y-\mu_y)^{\top}=var(Y)$
\end{emphbox}
\end{sectionbox}

\begin{sectionbox}
\subsection{Error Model Interpretation}
The projection is chosen in a way that the error measured with the Squared Euclidean Norm is minimized. The minimized noise is \emph{White Gaussian}.
\begin{itemize}
\item The error model interpretation is that the samples $\mathbf{X}$ is obtained via the addition between a signal matrix $\mathbf{L}$ (\emph{clean data}) and a noise matrix $\mathbf{N}$, where $\mathbf{L}$ is lying on a lower dimensional subspace than $\mathbf{X}$, and the goal is to try and find $\mathbf{L}$.
[$\mathbf{X}=\mathbf{L}+\mathbf{N}$]
\item Setting $\mathbf{L}$ equal to the SVD ($p$ dimensions with $k$; $rank(\mathbf{L})\le k$) minimizes the Frobenius norm of the difference between $\mathbf{X}$ and $\mathbf{L}$ given that the dimension $k$ of the subspace which $\mathbf{L}$ lies in is known beforehand. \\
$ \hat{\mathbf{L}}=\arg\min_{rank(\mathbf{L})}||\mathbf{X}-\mathbf{L}||_F=\mathbf{U}_k\mathrm{diag}(d_1,..,d_k)\mathbf{V}_k^{\top}$
\end{itemize}

\subsection{Relation to Autoencoders}
\begin{itemize}
    \item Autoencoders map an input to low dimensional space using a function $f$, which is then mapped back to a higher dimensional space using a function $g$ and can approximate the input $g\circ f(\mathbf{x_i})\approx\mathbf{x_i}$
\item Letting $f$ and $g$ be linear mappings represented by matrices, the reconstruction error $J(\mathbf{W,V})=\sum_{i=1}^n||\mathbf{x}_i-\mathbf{wVx}_i||^2$ is minimised by setting $f$ as the transpose of $U_k$ and $g$ as $U_k$.
\begin{emphbox}
    Let $\mathbf{U}_k$ be the first $k$ left singular vectors of observation matrix $\mathbf{X}$, the $\mathbf{V}=\mathbf{U}_k^{\top}$ and $\mathbf{W}=\mathbf{U}_k$ minimize the reconstruction error of the linear autoencoder.
\end{emphbox}
\item This is proven by noting that in Theorem 4.2 which has a problem of the same form, the error is minimized using the projections of the points on the subspace of interest, thus leading to $f$ and $g$ as the transpose of $U_k$ and $U_k$ itself respectively as they cause this projection.

\end{itemize}
\end{sectionbox}
\textbf{Notes:}
\begin{itemize}
    \item Low dimensional subspace $\hat{=}$ low rank
\end{itemize}

\vspace{300}
\section{Kernel PCA}
\begin{sectionbox}
\subsection{Linear PCA with Inner Products}
\begin{itemize}
\item Scores of the input data can be calculated using singular values and the right eigenvector (4.7), which we can obtain using the eigenvalue decomposition of the Gram-Matrix (7.2) and therefore giving us a way to calculate linear PCA scores using the inner product. If we forgot to center the data, use the centred Gram-Matrix instead (7.8).\\
\hspace{5}[$\mathbf{V}$ is orthogonal; $\mathbf{\Sigma}^{\top}\mathbf{\Sigma}$ is diagonal]
\item If the input data has not been centred beforehand, we can find the Gram-Matrix for the centred data by noting how input data is usually centred (7.6), factoring out the input matrix (7.7) and then using inner product to compute the Gram-Matrix  corresponding to centred input. (7.8)
\item Scores for new observations are usually calculated by projecting them onto $U_k$, but if we want to reformulate this to be in terms of the singular value and right singular vectors we can isolate U in the SVD and retain only k columns (7.4a) to arrive at the formulation. (7.4)
\item If we want to calculate scores for new observations considering centering, we take the formula for raw new observations (7.4), replace the input matrix with a centred input matrix and the observation with an approximated centred observation, resulting in a nice formula involving the Gram-Matrix. (7.9)
\begin{emphbox}
    EVD of Gram Matrix (7.2): $\mathbf{K=V\Sigma^{\top}\Sigma V^{\top}}$\\
    Centered Gram Matrix (7.8): $\mathbf{\tilde{K}=HKH}$ with $\mathbf{H}=(\mathbf{I}_n-\frac{1}{n}\mathds{1}_n\mathds{1}_n^{\top})$\\
    Computing scores for new sample $\mathbf{y}$:
    $\mathbf{U_k^{\top}}(\mathbf{\tilde{y}})=\mathbf{\Sigma_k^{-1}V_k^{\top}\tilde{k}_y}$, \quad $\mathbf{\tilde{k}_y=Hk_y-}\frac{1}{n}\mathbf{HK}\mathds{1}_n$
\end{emphbox}
\item With $\mathbf{V_k}$ and $\mathbf{\Sigma_k}=diag(\sigma_1, \cdots\sigma_k)$ are the first $k$ eigenvectors and eigenvalues of $\mathbf{K}$ we can compute the principal components $\mathbf{U}^{\top}_k\mathbf{y}$ by only using inner products.
\begin{emphbox}
    $\mathbf{U_k^{\top}}=\mathbf{\Sigma_k^{-1}V_k^{\top}X^{\top}}$ and for a new sample $\mathbf{y}$:\\
    $\mathbf{U_k^{\top}y}=\mathbf{\Sigma_k^{-1}V_k^{\top}X^{\top}y}=\mathbf{\Sigma_k^{-1}V_k^{\top}}\mat{\mathbf{x_1^{\top}y} & \cdots \mathbf{x_n^{\top}y}}^{\top}$\\
    This only requires the inner product $\mathbf{x_n^{\top}y=k_y}$
\end{emphbox}
\end{itemize}

\subsection{Transition to Kernel PCA}
\begin{itemize}
\item Kernel PCA requires us to first compute the Gram-Matrix via the kernel (6.1). This can then be substituted for the Gram-Matrix via inner product at various places, such as during centering (7.8) and when performing eigenvalue decomposition (7.2) to calculate matrices for scores (4.7).
\item The scores for new observations via Kernel PCA is based off the Linear PCA version (7.4), and can be obtained by replacing the linear observation inner products with the kernel observation inner products (as well as ensuring that singular values and right singular vectors obtained from eigenvector decomposition of Gram-Matrix via kernel (7.2)).
\item Similarly, when considering centering, note the Linear PCA version of scores which consider centering (7.9) and replace the linear Gram-Matrix with the kernel Gram-Matrix as well as the linear observation inner products with the kernel observation inner products. (7.12)
\end{itemize}
Instead of replacing the inner product $\mathbf{x^{\top}y}$ by $\langle\phi(\mathbf{x}),\phi(\mathbf{y})\rangle$, we substitute $\mathbf{x^{\top}y}\rightarrow\kappa(\mathbf{x,y})$:
\begin{emphbox}
    $\mathbf{k}^{new}=\mat{\kappa(\mathbf{x_1,y}) & \cdots & \kappa(\mathbf{x_n,y})}^{\top}$\\
    $\mathbf{k}_{cent}^{new}=\mathbf{Hk}_{}^{new}-\frac{1}{n}\mathbf{HK}\mathds{1}_n$
\end{emphbox}
\end{sectionbox}

\begin{sectionbox}
\begin{cookbox}{Kernel Principal Component Analysis}
    \item  for training set $\mathbf{X}=\mat{\mathbf{x}_1 & \cdots &\mathbf{x}_n}, \ \mathbf{x}_i\in\R^p$
	\item Find suitable Kernel function $\kappa(\cdot)$ and compute Gram Matrix
	\[
    \mathbf{K}=
    \begin{bmatrix}
    \kappa(\mathbf{x}_1,\mathbf{x}_1) & \dots  & \kappa(\mathbf{x}_1,\mathbf{x}_n) \\
    \vdots & \ddots & \vdots \\
    \kappa(\mathbf{x}_n,\mathbf{x}_1) & \dots  & \kappa(\mathbf{x}_n,\mathbf{x}_n)
    \end{bmatrix}
    \]
	\item Compute centered Gram Matrix: $\mathbf{\tilde{K}=HKH}$ with\\ $\mathbf{H=I}_n-\frac{1}{n}\mathds{1}_n\mathds{1}_n^{\top}$
	\item Compute Eigenvalue Decomposition: $\mathbf{\tilde{K}=V\Lambda V^{\top}}$. Because $\mathbf{K}$ is positive semi-definite and therefore the diagonal entries of $\mathbf{\Lambda}$ are non-negative, we write $\mathbf{\Lambda=\Sigma}^2=diag(\sigma_1^2, \cdots, \sigma_n^2)$
	\item Reduced Matrices: $\mathbf{\Sigma_k}=diag(\sigma_1,\cdots,\sigma_k),\ \in\R^{k\times k}$ and $\mathbf{V}_k\in\R^{n\times k}$
	\item Reduced Training Data: $\mathbf{S=\Sigma_kV}_k^{\top}$
	\item For new datapoint $\mathbf{y}\in\R^p$, compute new components: 
	\begin{center}
		$\mathbf{s}_new=\mathbf{\Sigma}_k^{-1}\mathbf{V}_k^{\top}\mathbf{k}_{cent}^{new}$\\
	    $\mathbf{k}_{cent}^{new}=\mathbf{Hk}_{}^{new}-\frac{1}{n}\mathbf{HK}\mathds{1}_n$\\
		$\mathbf{k}^{new}=\mat{\kappa(\mathbf{x_1,y}) & \cdots & \kappa(\mathbf{x_n,y})}^{\top}$
	\end{center}
\end{cookbox}
\end{sectionbox}
\textbf{Notes:}

\newpage
\section{Feedforward Neural Networks}
\begin{sectionbox}
\subsection{Definitions and Motivation}
The power behind an FNN is its ability to minimize any kind of expected prediction error, that is, find model parameters that minimize expected loss (5.1). Feed forward neural networks are concatenations of linear functions (5.2) with activation functions (e.g. 5.3), which can be denoted in vector form. This results in the generalized form of an FNN. (5.5)\\
With $p$/$N$ being Input/Output dimensions and $l$ layers. $N$ depends on loss function (Output of FNN is input of Loss). A \emph{Deep} FNN has $l>3$.
\begin{emphbox}
    Given function class $\mathcal{F}$ described by set of $N$ parameters $\Theta\in\R^N$:\\
    Solve: $\hat{\Theta}=arg\min_{\Theta\in\R^N}=\frac{1}{n}\sum_iL(f_\Theta(\mathbf{x}_i))$ (5.1)\\
    $\varphi_\mathbf{W}:\R^p\to \R^m, \mathbf{h}\to \mathbf{Wh}$ (5.2)\\
    $\sigma(t)=max\{0,t\}$ (ReLU, 5.3)\\
    $f:\R^p \rightarrow \R^o: \sigma_l\circ\varphi_{\mathbf{w}_l}\circ\cdots\circ\sigma_1\circ\varphi_{\mathbf{w}_1}(\mathbf{x})$ (5.5)
\end{emphbox}
\subsection{Training FNN}
The gradients can be calculated via chain rule through each subsequent layer, or in this case due to vectors via the Jacobian. From (5.5) the key stages include matrix multiplication (5.2) and nonlinear activation (5.3). The corresponding Jacobians w.r.t. inputs in each stage is seen in (5.6) and (5.7) respectively. However, for the Jacobian of matrix multiplication w.r.t weights, the weights need to be reshaped into a vector (5.9) which will then allow us to formulate the Jacobian (5.10). Then the gradients w.r.t loss (5.11) is simply (5.31a). The gradient can then be used to update the weights for the next time step, making sure to apply an inverse reshaping of the gradient vector into a matrix (5.14).
\begin{emphbox}
    $\mathbf{J}_{\mathbf{W} g}(\mathbf{x})=\mathbf{W\cdot J_g}(\mathbf{x})$ (5.6)
    $\mathbf{J}_{\sigma}(\mathbf{x})=diag(\sigma'(x_1),...,\sigma'(x_m))$
    (5.7)
$\mathbf{J}_{mult}(x)=diag(\mathbf{x}^{\top},...,\mathbf{x}^{\top})\in\R^{m\times mn}$ (5.10), doesn't depent on $\mathbf{W}$\\
With $\mathbf{h}_j=\sigma_j\circ\varphi_\mathbf{W_j}\circ\cdots\circ\sigma_1\circ\varphi_{\mathbf{W_1}}(\mathbf{x})$ the j-th output\\
We get for the output-layer (L): $\frac{\partial}{\partial\mathbf{W}_l}F=\mathbf{J}_L(\mathbf{h}_l)\cdot\mathbf{J}_{\sigma_l}(\mathbf{W_lh_{l-1}})\cdot\mathbf{J}_{mult}(\mathbf{h}_{l-1})$\\

$\frac{\partial}{\partial\mathbf{W}_j}F=\mathbf{J}_L(\mathbf{h}_l)\cdot\mathbf{J}_{\sigma_l}(\mathbf{W_lh_{l-1}})\cdot\mathbf{W_l}\mathbf{J}_{\sigma_{l-1}}(\mathbf{W_{l-1}h_{l-2}})\cdot\mathbf{J}_{l-1}\cdots \mathbf{J}_{mult}(\mathbf{h}_{l-1})$ for the j-th layer
\end{emphbox}

\subsection{Multiclass Classification with FNNs}
Considers the problem of assigning an input $\mathcal{X}\in\mathbb{R}^p$ to one out of multiple, say $C$ classes.
\begin{itemize}
    \item Model problem via random variable $(\mathcal{X,Y})$, with $\mathcal{X}\in\mathbb{R}^p$ and $\mathcal{Y}\in\{\mathbf{e}_1,...,\mathbf{e}_C\}$ as one of the standard basis vectors in $\mathbb{R}^C$.
    \item Realization $\mathbf{y}=\mathbf{e}_c$ means that the event \emph{belongs to class} $c$ is true.\\
    $\rightarrow$ This modelling of the output variable is also know as \emph{one-hot-encoding}.
    \item Idea: $\mathcal{X}$ is the input to the FNN and output vector $\mathbf{h}_l$ in $\mathbb{R}^C$ approximates the probability of the class distribution given $\mathcal{X}$.
    \[
    \mathbf{h}_l\approx
    \begin{bmatrix}
    Pr(\mathcal{Y}=\mathbf{e}_1|\mathcal{X}=\mathbf{x})\\
    \vdots\\
    Pr(\mathcal{Y}=\mathbf{e}_C|\mathcal{X}=\mathbf{x})
    \end{bmatrix}
    \]
    \item Thus last activation function $\sigma_l$ outputs probability distribution for the $C$ classes. All entries add up to 1.
    \item Example for output function is the \textbf{softmax}, given $\forall c\in1,...,C$ by
    \[
    \sigma:\mathbb{R}^C \to \mathbb{R}^C,
    \begin{bmatrix}
    a_1\\ \vdots\\ a_C
    \end{bmatrix}
    \to
    \begin{bmatrix}
    S_1\\ \vdots\\ S_C
    \end{bmatrix},\; S_c=\frac{\exp a_c}{\sum_c\exp a_c}
    \]
    \item Derivative/Jacobi matrix $J$ of softmax for arbitrary $i$ and $j$:
    \[
    J_{i,j} = \begin{cases}
        S_i(1-S_j) & ,i=j\\
        -S_j S_i & ,i\ne j
        \end{cases}
    \]
\end{itemize}
\end{sectionbox}

\section{Support Vector Machines}
\begin{sectionbox}
\subsection{Geometry}
Idea: Find hyperplane to divide $x, y$ into two subspaces
\begin{itemize}
\item For some vector $w$, an affine hyperplane is described as the set of points x that can be projected onto w and then shifted by some constant b to equal zero. (8.1)
\item The vector $w$ is normal to the hyperplane (8.1) since a vector of any direction in the hyperplane projected onto $w$ has a magnitude of zero. (8.2)
\item The signed distance from any point to a hyperplane is defined as in (8.3), and can be interpreted as the distance from the hyperplane to the point $x$. It is formulated by first projecting $x$ onto $w$ and then centering the number line at $b$ to obtain the signed value of $x$ in the decision perspective (denote this as the decision distance). Then this value is divided by the magnitude of w to obtain the signed distance. Note that $-b$ denotes the decision distance to the origin, and the magnitude of $w$ is a scaling factor used to shift between the decision distance and signed distance.
\begin{emphbox}
    Hyperplane:$\mathcal{H}_{\mathbf{w},b}=\{x\in \mathbb{R} | \mathbf{w}^{\top}\mathbf{x}-b=0\}$ (8.1)\\
    $w$ is normal to $\mathcal{H}$: $\mathbf{w}^{\top}(\mathbf{x}_1-\mathbf{x}_2)=0$ (8.2)\\
    Signed Distance: $\delta(\mathbf{x}, \mathcal{H}_{\mathbf{w},b})=\frac{\mathbf{w^{\top}x}-b}{||\mathbf{w}||}$ (8.3)\\
    Margin:$\mathcal{H}_{+}=\{x\in \mathbb{R} | \mathbf{w}^{\top}\mathbf{x}-b=1\}$ $\mathcal{H}_{-}=\{x\in \mathbb{R} | \mathbf{w}^{\top}\mathbf{x}-b=-1\}$ (8.8-9)
\end{emphbox}
\item The decision perspective can be imagined as the perspective where classification decisions are made, all depending on the decision distance.
\item The positive margin is defined as the hyperplane with a decision distance of 1 (8.8), and the negative margin is defined as the hyperplane with a decision distance of -1 (8.9).
\item Decision boundary of SVM is always a hyperplane (i.e. an affine space) in $\mathbb{R}^p$.
\item Retraining with a new training sample affects the decision boundary.
\end{itemize}

\subsection{Basic Linear SVM}
\begin{itemize}
\item The aim of Linear SVM is to linearly separate data using the margins of the hyperplanes, such that all positively labelled data is bounded by the positive margin and all negatively labelled data is bounded by the negative margin. In other words, all positively labelled data has a decision distance greater than or equal to 1, and all negatively labelled data has a decision distance less than or equal to -1, resulting in a separation constraint for $w$ and $b$. (8.12)
\item We define the best $w$ and $b$ as the ones that have the widest margin, i.e. the widest distance between the hyperplanes. This width his calculated by the sum of the signed distance from the positive margin to the hyperplane and the signed distance from the hyperplane to the negative margin (8.13), which we can then maximize subject to our separation constraint (8.12). 
\item Flipping around the numerator and denominator of the width, this becomes a minimization problem (8.14).
\begin{emphbox}
    Conditions: $\mathbf{w^{\top}x}_i-b\ge +1 \ \mbox{for } y_i=
    1$ and $\mathbf{w^{\top}x}_i-b\ge -1 \ \mbox{for } y_i=
    -1$\\
    Compact: $y_i\cdot(\mathbf{w^{\top}x}_i-b)\ge1 \ \text{for all } i=1,...,N$ (8.12)\\
    Minimization (8.14): $\min\limits_\mathbf{w}\frac{1}{2}||\mathbf{w}||^2_2 \ \mbox{s.t.} \ y_i(\mathbf{w^{\top}x}_i-b)\ge1 \ \text{for all } i=1,...,N$
\end{emphbox}
\end{itemize}
\end{sectionbox}

\begin{sectionbox}
\subsection{Karush-Kuhn Tucker Conditions}
\begin{itemize}
\item The KKT conditions say that a minimisation problem with a set of equality and inequality constraints (8.15) can be reformulated as a Lagrangian primal (8.16) with some KKT conditions (Theorem 8.2), such that for convex objective functions with a convex feasible region (like that defined 8.14) minimising the primal (8.16) is equivalent to minimising the original problem (8.15).
\begin{emphbox}
    For a optimization problem with $\mathcal{E}$ and $\mathcal{I}$ as \emph{equality and inequality constraints} (8.15):\\
    $\min\limits_{\mathbf{z}\in\R^n}f(\mathbf{z}) \ \mbox{s.t. } c_i(\mathbf{z}) = 0 \ \forall i\in\mathcal{E} \ \mbox{and s.t. } c_j(\mathbf{z}) \ge 0 \ \forall j\in\mathcal{I}$\\
    Lagrange Function (8.16): $L(\mathbf{z,\lambda})=f(\mathbf{z})-\sum\limits_{i\in\mathcal{I}\bigcup\mathcal{E}}\lambda_ic_i(\mathbf{z})$
    \textbf{Karush-Kuhn-Tucker Conditions}\\
    For $\mathbf{z^*}$ as a solution to 8.15, there exists a Lagrange multiplier $\mathbf{\lambda^*}$ such that (8.17-8.21):\\
    \begin{itemize}
        \item $\nabla_zL(z^*,\lambda^*)=0$
        \item $c_i(z^*)=0 \quad \forall i\in\mathcal{E}$
        \item $c_i(z^*)\ge0 \quad \forall i \in\mathcal{I}$
        \item $\lambda_i^* \ge0 \quad \forall i\in\mathcal{I}$
        \item $\lambda_i^*c_i(z^*)=0 \quad \forall i\in\mathcal{I}\bigcup\mathcal{E}$
    \end{itemize}
\end{emphbox}
\item The convex primal function (8.16) can then be reformulated as a concave dual function (8.22) by taking the infimum of the primal function. The infimum is the set of points along which the function is minimized w.r.t. the non-Lagrangian multiplier variables. Then by maximizing the primal subject to its constraints (8.22a) we obtain a lower bound for the solution to the primal (weak duality) or when some conditions are satisfied (such as in SVM) this coincides with the solution to the primal (strong duality).
\end{itemize}

\subsection{Linear SVM via Lagrangian Duality}
\begin{itemize}
\item SVM via Lagrangian duality follows the process specified above. The original problem is as in (8.14), the Lagrangian primal is as in (8.23 to 8.25) and the KKT conditions are as in (8.17 to 8.21). 
\begin{emphbox}
    Problem: $\min\limits_{\mathbf{w}, b, \mathbf{\lambda}\ge0} L(\mathbf{w}, b, \mathbf{\lambda})$ \\  $L(\mathbf{w}, b, \mathbf{\lambda})= \frac{1}{2}||\mathbf{w}||^2 - \sum\limits_i \lambda_i y_i(\mathbf{w^{\top}x_i}-b)+\lambda_i $ (8.25)\\
    		$\nabla_{(\mathbf{w},b)}L(\mathbf{w}, b, \mathbf{\lambda})=\mat{\mathbf{w}-\sum_i\lambda_iy_i\mathbf{x}_i \\ \sum_i \lambda_iy_i}$\\
    KKT Conditions:\\ $\mathbf{w^*}-\sum_i\lambda_i^*y_ix_i=0$ (8.27), \quad $\sum_i\lambda^*_iy_i=0$ (8.28)\\
    $\lambda_i^*(y_i(w^{*T}x_i-b^*)-1=0$ (8.29)\\
    Returns (8.30): \\
    $\min\limits_{\mathbf{w}, b, \mathbf{\lambda}\ge0} L(\mathbf{w}, b, \mathbf{\lambda})=L_D(\mathbf{\lambda})=
    \sum\limits_i\lambda_i-\frac{1}{2}\sum\limits_{i,j}\lamda_i\lambda_jy_iy_j\mathbf{x_i^{\top}x_j}$ s.t. $\lambda_i\ge0, \ \sum_i\lambda_iy_i=0$
\end{emphbox}

\item Then the dual function needs to be calculated by taking the infimum (\emph{greatest element}) of (8.25), which is accomplished by substituting (8.27) and then (8.28) into (8.25), resulting in the dual form (8.30).
\item Maximising this function w.r.t. its constraints is then the dual problem for the SVM. (8.30)

\item Problem is strictly convex, therefore solution is unique
\item If $\lambda_i \ne 0$, then $\mathbf{x}_i$ is support vector (Lies in or on margin)
\item Only works, if classes can be linearly separated \\
Else: Kernel/Soft Margin SVM required
\item SVM works better when (nearly) separable than Linear Regression and is preferred when using kernels.
\end{itemize}

\subsection{Soft Margin Linear SVM}
Allows for some wrongly assigned data samples and searches for a trade-off between a large margin and a degree of misclassification.
\begin{itemize}
    \item Misclassification is quantified by set of $N$ additional variables $\xi_i$ that are assumed to be non-negative.\\
    $y_i(\mathbf{w}^{\top}x_i-b)\ge 1-\xi_i)$, for all $i=1,...,N$
    \item Optimization problem changes to\\
    $\min_{\mathbf{w},b,\xi}\frac{1}{2}||\mathbf{w}||_2^2+c\sum_{i=1}^N\xi_i$\\
    s.t. $y_i(\mathbf{w}^{\top}x_i-b)\ge 1-\xi_i)$ and $\xi_i\ge0\;\;\forall i$\\
    \hspace{5}[ $c>0$\emph{ and weighs between large margin and misclassification }\\$\rightarrow$\emph{ larger }$c$\emph{, the violation of the separation rule is punished more.}]
\end{itemize}
\end{sectionbox}

\begin{sectionbox}
\subsection{Kernel SVM}
\begin{itemize}
    \item Dual Form: \\$\max_{\lambda}\sum_i \lambda_i - \frac{1}{2}\lambda^{\top}\mathbf{H}\lambda$ s.t. $0\le\lambda_i\le c, \sum_i \lambda_i y_i=0$\\
with $h_{ij}=y_i y_j \mathbf{x}_i^{\top} \mathbf{x}_j$ which changes to $h_{ij}=y_i y_j \kappa(\mathbf{x}_i, \mathbf{x}_j)$ using a Kernel function.
\item Using the KKT conditions, we can compute\\$b=\frac{1}{N_{Supp}}\sum_{i\in Supp}(\sum_{j\in Supp}(\lambda_j y_j \kappa(\mathbf{x}_i,\mathbf{x}_j))-y_i)$
\item $Supp$ denotes all $i$ for which $0<\lambda_i\le c$.
\end{itemize}

\end{sectionbox}


\section{Useful Facts}
\begin{itemize}
    \item The matrix resulting from the dot product of between an $\mathbb{R}^{n \times p}$ matrix and $\mathbb{R}^{p \times n}$ has at most a rank of p.
    \item Examples for Loss-functions:\\
    \textbf{Autoencoder}: $L(f(x))=||f(x)-X||_2^2$\\
    \emph{reconstruct samples of} $x$\\ 
    $f$:\emph{ concatenation of encoders and decoder function}\\
    \textbf{Regression}: $L(f(x), Y)=||f(x)-Y||_2^2$\\
    \emph{best} $f \in F$ \emph{that minimizes expectation of }$L$
    \item \textbf{Unsupervised Learning}: data does not have to be labeled (by supervisor) before data is employed for learning algorithm (e.g. PCA).
    \item \textbf{Supervised Learning}: data is labeled and thus the is model is trained $\rightarrow$ Classification vs. Regression
    \item \textbf{Regression}: predicting trends using previous labeled data
    \item $cov(x_1,x_2)=\frac{1}{n}XX^{\top}=\frac{(x_{11}x_{21}+x_{12}x_{22}+...)}{n\cdot||x_1||\cdot||x_2||}$
    \item \textbf{Data Scales}:\\
    \emph{Qualitative}: Nominal ($=,\neq\;\rightarrow$ Gender or Names), \\Ordinal ($\ge,\le,>,<\;\rightarrow$ Happiness)\\
    \emph{Quantitative}: Interval ($x,-\;\rightarrow$ 2019 or 18Â°C), \\Ratio ($ * , \frac{\cdot}{\cdot} \;\rightarrow$ Distance or Area)
    \item A function $f$ is called a \textbf{Distance Measure} if $\forall x,y \in \mathbb{R}^p$:\\
    \emph{Symmetry}: $d(x,y)=d(y,x)$\\
    \emph{Positive Definiteness}: $d(x,y)=0 \Leftrightarrow x=y; d(x,y)\ge0$\\
    \emph{Triangular Irregularity}: $d(x,z)\le d(x,y)+d(y,z)$
    \item \textbf{Taxicab/Manhattan Norm}: $||\mathbf{x}||_1=\sum_{i=1}^n{x_1}$\\
    \textbf{Euclidean Norm}: $||\mathbf{x}||_2=\sqrt{x_1^2+...+x_n^2}$\\
    \textbf{Maximum Norm}: $||\mathbf{x}||_{\infty}=max(|x_1|,...,|x_n|)$\\
    \textbf{Inner Product Norm}: $||\mathbf{x}||_A=\sqrt{\mathbf{x}A\mathbf{x}^{\top}}$  [Frobenius: $A=\mathbbm{1}$]\\
    \emph{Reminder}: $||q\cdot\mathbf{x}||=|q|\cdot||\mathbf{x}||$ and $||\mathbf{x}+\mathbf{y}||\le||\mathbf{x}||+||\mathbf{y}||$
    \item \textbf{Lagrange Method}:\\
    1. $L(x,y,\lambda)=f(x,y)+\lambda\cdot g(x,y)$ \\($g(x,y)$ \emph{is the conditional equation set to zero})\\
    2. $\bigtriangledown L\overset{!}{=}0$ (\emph{calculate} $L_x$(\textrm{I}), $L_y$(\textrm{II}) and $L_{\lambda}\overset{!}{=}0$ (\textrm{III}))\\
    3. Eliminate $\lambda$ from $L_x$ \& $L_y$, (\textrm{IV}): $\frac{df(x,y)}{dx}\cdot y-\frac{df(x,y)}{dy}\cdot x\overset{!}{=}0$\\
    4. Solve system of equations (\textrm{III}) and (\textrm{IV}) to find all $x$ and $y$.
    \item Training and test sets normally get divided into:\\
    \emph{Machine Learning}: 60/20/20\\
    \emph{Deep Learning}: 98/1/1
\end{itemize}


\newpage
\section{Homework and Assignments}
Given a prediction table, eg:
\begin{tabular}{l|c|r}
    $p_X(X_1,X_2)$& $X_2=0$ & $X_2=1$ \\
    \hline
     $X_1=0$& $p_X(0,0)$& $p_X(0,1)$ \\
     \hline
     $X_2=1$& $p_X(1,0)$& $p_X(1,1)$
\end{tabular}
    \begin{cookbox}{Calculate Covariance Matrix from Table}
            \item Calculate Means for $X_1$ and $X_2$: $\mu_1, \mu_2$
            \item Create X-Matrix, e.g.: \mat{0 & 1 & 0 & 1\\ 0 & 0 & 1 & 1}
            \item Create p-Matrix: \mat{$p_X(0,0)$ & 0 & 0 & 0 \\
                                        0 & $p_X(1,0)$ & 0 & 0 \\
                                        0 & 0 & $p_X(0,1)$ & 0 \\
                                        0 & 0 & 0 & $p_X(1,1)$}
            \item Calculate Covariance: $Cov=XpX^{\top}$
    \end{cookbox}
\begin{sectionbox}
\subsection{Classification Analysis}
\emph{False Positive}: Indicates presence of a condition during a test, but condition is actually \textbf{not} present.\\
\emph{False Negative}: Indicates \textbf{no} presence of a condition, actually \textbf{is} present.
\emph{True Positive}: Indicates presence of a condition, actually \textbf{is} present.\\
\emph{True Negative}: Indicates \textbf{no} presence of a condition, actually \textbf{not} present.
\begin{center}
	\includegraphics[width = 1.0\columnwidth]{figures/precession_recall.png}
\end{center}
\begin{itemize}
    \item \textbf{Accuracy}: $\frac{\text{# correct predictions}}{\text{# total data points}}$\\
    Drawbacks: Cost of misclassification might differ for different tasks \& we have a lot of data for one class but not the other (\emph{Skewed Data}).
    \item \textbf{Precision}: $\frac{\text{# happy correct answers}}{\text{total items returned by ranker}} = \frac{\text{True Positives}}{\text{True Positives + False Positives}}$
    \item \textbf{Recall}: $\frac{\text{# happy correct answers}}{\text{total relevant items}} = \frac{\text{True Positives}}{\text{True Positives + False Negatives}}$
    \item \textbf{F1-Score}: $F_1= 2*\frac{\text{precision*recall}}{\text{precision+recall}}$
    \item \textbf{NDCG} (Normalized Discounted Cumulitive Gain):\\
    Similar to a search engine, top few answers in a query matter more.
    \emph{Related metrics}: Cumulative gain sums up the relevance of the top $k$ items. Discounted cumulative gain (DCG) discounts items that are further down the list. Normalized discounted cumulative gain is a normalized version of DCG which divides the DCG by the perfect ideal  DCG score with ideal ordering, so that the normalized score always lies between 0.0 and 1.0. For a position $p$:
    $DCG_p=\sum_{i=1}^p\frac{2^{rel_i-1}}{log_2(i+1)}$ \hspace{5}[for IDCG $p=|REL|$]
\end{itemize}
\subsubsection{ROC Curve}
The ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It shows:
\begin{itemize}
    \item The relationship between sensitivity and specificity. For example, a decrease in sensitivity results in an increase in specificity.
    \item Test accuracy; the closer the graph is to the top and left-hand borders, the more accurate the test. Likewise, the closer the graph to the diagonal, the less accurate the test. A perfect test would go straight from zero up the the top-left corner and then straight across the horizontal.
    \item The likelihood ratio; given by the derivative at any particular cutpoint.
    \item \emph{False Positive Rate}: $\frac{\text{False Positives}}{\text{False Positives + True Negatives}}$\\
    \emph{True Positive Rate}: $\frac{\text{True Positives}}{\text{True Positives + False Negatives}}$
\end{itemize}
\end{sectionbox}

\begin{sectionbox}
\begin{center}
	    \includegraphics[width = 0.9\columnwidth]{figures/sphx_glr_plot_roc_001.png}
	\end{center}
The \textbf{Area under the Curve (AUC)} is the integral of the ROC Curve and gives a measure how good the classifier is. An area under the ROC curve of 0.8, for example, means that a randomly selected case from the group with the target equals 1 has a score larger than that for a randomly chosen case from the group with the target equals 0 in 80\% of the time. When a classifier cannot distinguish between the two groups, the area will be equal to 0.5 (the ROC curve will coincide with the diagonal). When there is a perfect separation of the two groups, i.e., no overlapping of the distributions, the area under the ROC curve reaches to 1 (the ROC curve will reach the upper left corner of the plot). 

\subsection{Curse of Dimensionality}
\begin{itemize}
    \item The angular distance between $2$ randomly sampled vectors increases with dimension $d$ of the sample space.
\item Convergence to $\frac{\pi}{2}$ implies that two randomly sampled vectors are orthogonal to each other in $d$-dimensional apsce for $d\gg n$.
\item Convergence to $\frac{\pi}{2}$ also implies that most samples are concentrated in the 'corners' of the $d$-dimensional cube $[-1, 1]^d$, i.e. in high dimension, the corners occupy most of the space.
\item This convergence also means that $2$ randomly sampled vectors are increasingly equidistant (in termas of angular distance) from their respective nearest neighbors in high dimensional space.
\item Because the samples are increasingly equidistant from each other, this means that distance-based classifiers (e.g. k-Nearest Neighbors) cannot be used on such data in high-dimensional space.
\item Increasing the sample size $n$ decreases the average angular distance between neighbouring vectors in a $d$-dimensional feature space. The rate of decrease, however, decreases with increasing $n$.
\end{itemize}

\begin{center}
	    \includegraphics[width = 0.9\columnwidth]{figures/angular_distance_plot.png}
	\end{center}

 \subsection{Logistic Regression}
 \begin{itemize}
     \item  With big datasets, standard gradient descent could lead to       \texttt{Memory Error}
     \item Use \emph{stochastic gradient descent} instead: Train over epochs instead:
     \item Each epoch, the training set is divided randomly into equal size subsets (=minibatch). Then the gradient of each subset is calculated and applied only to the samples in the subset
     \item A epoch is finished when the gradient step was performed on each subset

 \end{itemize}
\end{sectionbox}

\begin{sectionbox}
\subsection{Principal Component Analysis}
Removing the first $n$ columns from $U_k$ can have different effects on classification:
\begin{itemize}
    \item \textbf{Decreased Error Rate}: This may be because even though the first $n$ components capture more variance in the samples, perhaps the other components are better at separating samples by labels, allowing KNN to correctly classify samples (Subset 1 in top plot and second plot)
    \item \textbf{No Effect on Error Rate}: This may be because the first three principal components are as good at separating samples by labels compared to other principal components (Subsets 2+3 in top plot and third plot)
    \item \textbf{Increase Error Rate}: This may be because the first three principal components are better at separating samples by labels compared to the other principal components (Subset 4 top plot and bottom plot)
\end{itemize}
\subsubsection{How to choose $k$?}
Assuming that $\mathbf{X} \in \mathbb{R}^{p \times N}$ is the centered data matrix and $\mathbf{P} = \mathbf{U}_k \mathbf{U}_k^\top$ is the projector onto the $k$-dimensional principal subspace, the dimension $k$ is chosen such that the fraction of overall energy contained in the projection error does not exceed $\epsilon$, i.e.\
	\begin{equation*}
	    \frac{\|\mathbf{X} - \mathbf{PX}\|_F^2}{\|\mathbf{X}\|_F^2} 
	    = \frac{\sum_{i=1}^{M} \|\mathbf{x}_i - \mathbf{Px}_i\|^2}{\sum_{i=1}^N \|\mathbf{x}_i\|^2} \leq \epsilon,
	\end{equation*}
where $\epsilon$ is usually chosen to be between 0.01 and 0.2. Energy is not always the best way to measure useful information, e.g. when images differ in brightness (=No use full information)
\end{sectionbox}

\begin{sectionbox}
\subsection{Exam Winter Term 18/19}
The questions where completely different to what was discussed in the lecture, the assignments and the tutorial. They focused on logical thinking and simple calculations. Questions had a structure like such:\\
"These six different $x_i$ and $y_i$ are given. Which of the following $\mathbf{w}$ and $\lambda$ are correct?"\\
$\rightarrow$ Solution: Use the KKT Conditions with each possible $\mathbf{w}^*$ and $\lambda^*$ to see which one has a plausible solution.\\
"Which of the following could (not) be a Kernel function?"\\
$\rightarrow$ Solution: Check is $K$ is positive semidefinite, if $\kappa$ is symmetric etc.\\
Most questions could be solved by using the method of elimination and almost all the time the information necessary to solve the question was in the description of the question (\textit{e.g. they stated the KKT Conditions}).
\end{sectionbox}
% DOCUMENT_END =================================================================
\end{document}
